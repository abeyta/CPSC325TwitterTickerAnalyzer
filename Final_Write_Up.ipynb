{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adrian Abeyta Final Write-Up\n",
    "## Twitter Ticker Analyzer\n",
    "### Getting Started\n",
    "From the start of the semester, I wasn't quite sure which direction I would take with my project. Once I figured out that I wanted to make a stock application with twitter data I started looking into GCP solutions and the yahoo finance api for gathering stock data. Along the way I ran into sentiment analysis of tweets and figured it would be intersting to see if there was a positive correlation between stock price movement and tweet sentiment. I began researching sentiment analysis models such as BERT and VADER and found that BERT was easier to integrate with GCP and would be applicable for future projects. \n",
    "\n",
    "### Roadblocks\n",
    "A major raodblock I encountered was getting my BERT model to work on GCP. Originally I wanted to train my model and do everything with the predictions through GCP's native BERT function under the AI Platform, but was unable to due to a resource error and multiple hours of troubleshooting. Through this I happened to start a TPU node that required many credits from our $50 allocated so I had to set up my credit card for the remaining month of the project. Another large roadblock I encountered towards the end of my project is serializing the tensors that are generated by the BERT model in order to pass them as a JSON in the HTTP response from the cloud function. I needed these tensors to be able to plot the sentiment points on my application for the user. \n",
    "\n",
    "### Data\n",
    "Since this is a data-focused project, I wanted to focus on twitter data that get scrapped on every prediction. I figured that the data couldn't get more timely than this. I wanted to have the most accurate prediction possible because I know the stock market can move very quickly. After my first project update presentation I realized I had to remove spam tweets and other symbols in the tweets that might be detrimental to determining the sentiment. To do this I removed all tweets with links becuase I found that most spam tweets were linking to outside websites promoting their own stock data, and also removing tweets that mentioned more than 3 stock tickers in a single tweet. \n",
    "\n",
    "### Deployment\n",
    "At the beginning of my project I planned to host everything on GCP's App Engine, but after running into some errors I realized I needed to move my BERT predictions to a different section of GCP. This is where I used your help and worked with Payton for a few hours to finally get my BERT model loaded into a cloud function from a storage bucket. This process was very time consuming because I had to constantly redeploy the cloud function to test each time and once the cloud function got larger and more complex it would take 5-10 minutes on each deploy. Deploying the application in the App Engine was very straight forward after reading the documentation and learning what it was capable of. When running my app it takes a long time, relative to the rest of the application, to make a prediction because it has to download the BERT model each time and then start from there to make the prediction.\n",
    "\n",
    "### Testing\n",
    "I didn't write any unit tests for the project because I didn't feel as they were necessary for a project with many api calls. Looking back I could have written some unit tests for my data cleaning to make sure they were removing parts of text that I wanted. Besides that, I manually tested my cloud functions by deploying each time and testing the function through the GCP console. Validating the usability of the project is something I still am continuing to do by making predictions and recording the difference in the open price versus close price of the stock. If I were to continue this project I would add a history page to the application where users would be able to see the accuracy history for each stock ticker.  \n",
    "\n",
    "### Datasets and Models\n",
    "To train my BERT model for sentiment analysis I followed a very similar process to my tutorial presentation. For my presentation I used a very small batch size and only 1 epoch for the demonstration to be completed in class. For my actual BERT model I increased the batch size to 20,000 and tried to train over 2 epochs on my computer. This process took many hours and while running overnight my computer froze and stopped during the second epoch. I saved a checkpoint of the model after the first epoch getting 82% accuracy and figured it was a large improvement over my 44% model I was using for testing before. To train the model I used the Stanford movie review dataset linked here: https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz. When I first planned my project I wanted to encorporate historical data into my model to increase accuracy, but I found this to be very difficult since I would have to retrain my entire BERT model each time I wanted to do this and manually classify each tweet I would keep as positive or negative.\n",
    "\n",
    "### Originality\n",
    "To make this project original I looked at twitter data to make my predictions on whether the stock would go up or down. It's similar to some of the other stock apps in the class because I am only predicting up or down and not a percent increase or decrease. One way my project is original compared to Sam's is I am looking at tweets for my prediction unlike his project where he looked at discord and news articles. I figured tweets would give me the most current sentiment on a stock because it's really easy to write out 256 characters about how you feel trading the stock is going. Since I was only looking at daily improvements I avoided articles because those tend to be released after the trading period has closed and relate to opinions looking back on the day or outlooks for the longer future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f59d380de1c7674f3c68848718fa932d1af0c4a6d2ed9bf7f4860688d45cde6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
